% To compile: pdflatex file.tex
\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{pgffor}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{verbatim}
\usepackage{appendix}
\usepackage{graphicx}
\usepackage{color}
\usepackage{subfig}
\usepackage{url} % for underscore in footnote
\usepackage[UKenglish]{isodate} % for: \today
\cleanlookdateon                % for: \today
\usepackage{natbib} % Can remove if no bibliography. bibtex
%\pagestyle{empty} % Removes page number. Graphs too big.

\def\wl{\par \vspace{\baselineskip}\noindent}
% Keeping Figures in the right place %%%%%%%
\usepackage{float}                        %%
\def\beginmyfig{\begin{figure}[H]\center} %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\endmyfig{\end{figure}}
\def\ds{\displaystyle}
\def\tu{\textunderscore}
\definecolor{grey}{rgb}{.2,.2,.2}
\definecolor{lgrey}{rgb}{.8,.8,.8}
\def\hline{ \textcolor{lgrey}{\hrulefill} }
\newcommand{\m}[1]{\mathbf{\bm{#1}}} % Serif bold math
\def\ds{\displaystyle}                                                    
\def\inv{^{\raisebox{.2ex}{$\scriptscriptstyle-1$}}}
%\def\pm{^{\raisebox{.2ex}{$\scriptscriptstyle\prime$}}}
\def\norm#1{\left\lVert#1\right\rVert}
\newcommand{\p}[1]{\left(#1\right)}
\newcommand{\bk}[1]{\left[#1\right]}
\newcommand{\bc}[1]{ \left\{#1\right\} }
\newcommand{\abs}[1]{ \left|#1\right| }
\newcommand{\mat}{ \begin{pmatrix} }
\newcommand{\tam}{ \end{pmatrix} }

% \def for THIS ASSIGNMENT!!!%%%%%%%%%%%%%%%%%%%

\begin{document}
% my title:
\begin{center}
  {\huge \textbf{Take Home Final (AMS 205B)}
    %\footnote{\url{https://github.com/luiarthur/bnp_hw/project/bnp_spatialDP}}
  }\\
  \wl
  \noindent\today\\
  Arthur Lui\\
  \hline
\end{center}


\def\ci{\cos(wx_i+\delta)}
\def\si{\sin(wx_i+\delta)}
\def\hi{\alpha-\beta\ci}
\def\hci{\cos(\hat w x_i+\delta)}
\def\hsi{\sin(\hat w x_i+\delta)}
\def\hhi{\alpha-\beta\hci}
\def\suml{\sum_{i=1}^n}
\def\mlevec{(\alpha,\beta,w,\delta,\sigma^2)}
\def\hmlevec{\hat\alpha,\hat\beta,\hat w,\hat\delta,\hat\sigma^2}

\subsection*{1a)}
The MLE for ($\alpha,\beta,w,\delta,\sigma^2$) = $\underset{\alpha,\beta,w,\delta,\sigma^2}{\text{argmax}}\log f(\bm y|\bm x,\alpha,\beta,w,\delta,\sigma^2)$.
\[
\begin{aligned}
  &= \underset{\alpha,\beta,w,\delta,\sigma^2}{\text{argmax}}\bc{-\frac{n}{2} \log(\sigma^2) - \frac{\sum_{i=1}^n \p{y_i-\alpha-\beta \cos\p{wx_i+\delta}}^2}{2\sigma^2}}\\
\end{aligned}
\]

\noindent
The MLE is not available in closed form because the MLE's of $w$ and $\delta$ cannot be obtained in closed form. For example, to calculate the MLE of $w$ is obtained as follows:\\

\noindent
Let $l$ be the log likelihood $\log f(x|\alpha,\beta,w,\delta,\sigma^2)$. Then
\[
\begin{aligned}
  l_w &=  -\beta\si\frac{\suml\p{y_i-\hi}}{\sigma^2}\\
  l_w &:=0\\
  \Rightarrow -\beta\hsi\frac{\suml\p{y_i-\hhi}}{\sigma^2}&=0\\
\end{aligned}
\]

\noindent
So, there is no obvious closed form solution for $\hat w$, and hence \textbf{no closed form expression for the MLE of} ($\alpha,\beta,w,\delta,\sigma^2$). Similarly, no closed form solution is available for the MLE of $\delta$.\\

\noindent
With some algebra and linear model theory, the MLE's for $\alpha$, $\alpha$ and $\beta$ can be computed to be:
\[
\begin{aligned}
  \hat\sigma^2 &= \frac{\suml\p{y_i-\hi}^2}{n}\\
  \begin{pmatrix} \alpha\\\beta\\ \end{pmatrix} &= (C'C)^{-1}C'\mathbf y
\end{aligned}
\]
\noindent where $C=\begin{pmatrix}\mathbf 1 & \mathbf x\end{pmatrix}$.\\

\noindent The MLE of $\mlevec$ can be solved for by optimizing the log
likelihood profile with respect to $w$ and $\delta$. This was implemented in R
using the ``optim" function with a grid of initial values and the negative
likelihood profile as the function to minimize. The hessian matrix was also
returned to compute the second derivaties for computing the standard errors
later. The grid of initial values to test was determined by plotting the
bivariate log likelihood profile evaluated at $w,\delta$. It can be seen from
Figure \ref{fig:grid} that the maximum occurs periodically along $\delta$ and
at $w$ between 1.7 and 1.8. The support of $\delta$ was picked to be
($-\pi,\pi$) since that is most logical for testing the hypothesis in the later
problem. The $100 \times 100$ grid points plotted in Figure \ref{fig:grid} were used as
starting values for the optimizer. The $(w,\delta)$ pair that maximized the
likelihood profile was $(1.75, -.0373)$. \textbf{The resulting MLE in the
orbits data set for} $\mlevec=(1.97,-1.50, 1.75, -.0373, .0198)$.

\beginmyfig
  \includegraphics[scale=.5]{../code/output/grid.pdf}
  \caption{\small Log likelihood profile evaluated at various $\delta$ and $w$ values. log likelihoods range from -58 to 147. Regions in darkred correspond to a higher log likelihood.}
  \label{fig:grid}
\endmyfig

\noindent
New data was simulated using the MLE's to visually examine the goodness of fit.

\beginmyfig
  \includegraphics[scale=.5]{../code/output/mle.pdf}
  \caption{\small Fitted model plotted against data.}
  \label{fig:mle}
\endmyfig

\subsection*{1b)}
In order to use the wald's test normal approximation, we need to check the 6 regularity conditions. 

\begin{itemize}
  \item[1.]{Under the model assumptions, $\epsilon_i \sim N(0,\sigma^2)$, so we have iid observations.}
  \item[2.]{The parameters are identifiable as long as constraints are appropriately defined. Specifically, when $w$ is positive and $\delta$ is bound between $-\pi$ and $\pi$, the parameters are identifiable.}
  \item[3.]{The support of the likelihood is indeed independent of the parameters.}
  \item[4.]{In this problem, when $w$ and $\delta$ are at the bounds, it is not practical or meaningful to estimate the parameters. When the frequency $w$ is 0, we don't see a relationship between $x$ and $y$. It may be the case that the true $\delta$ is $\pi$ or $-\pi$. But in problems involving seasons in trends, usually someone with subject matter expertise can inform us of possible trends. So we can avoid modelling problems with a shift parameter at the boundaries.}
  \item[5.]{The likelihood involves linear terms and $w$ and $\delta$ are associated with a trigonometric function in a way such that it is infinitely differentiable with respect to $w$ and $\delta$.}
  \item[6.]{Not sure what to show here...}
\end{itemize}

\noindent
Since the regularity conditions hold, 95\% Confidence intervals for $w$ and
$\delta$ can be constructed by inverting a Wald-like test can be obtained by
computing
\[
\begin{aligned}
  w &= \hat w \pm z_{.025} SE(\hat w) \\
  \delta &= \hat \delta \pm z_{.025} SE(\hat \delta) \\
\end{aligned}
\]
where $\hat w$ and $\hat\delta$ are the MLE's for $w$ and $\delta$
respectively. The standard errors for $\hat w$ and $\delta$ can be computed as
$\bk{I(\hat w)}^{-1/2}$ and $\bk{I(\hat\delta)}^{-1/2}$, where $I(\hat w)$ and
$I(\hat \delta)$ are the observed inverse informations. The observed inverse
informations are obtained from the hessian output of the optim function used in
estimating the MLE's. The following are the results.\\

\noindent
A 95\% Confidence interval for $w$ based on the Wald-like test is (\textbf{1.749, 1.756}).\\
A 95\% Confidence interval for $\delta$ based on the Wald-like test is (\textbf{-0.06396, -0.01061}).

\subsection*{1c)}
A parametric bootstrap algorithm can be implemented in the following way:\\

\begin{itemize}
  \item Initialize $(\hmlevec)^{(0)} = (\hmlevec)$.
  \item Repeat for $b$ = $1$ to $B$, some big number
  \begin{itemize}
    \item Draw $\bm y^{(b)} \sim \bm f(y|\bm x,\hmlevec)$
    \item set $(\hmlevec)^{(b)}= \underset{\alpha,\beta,w,\delta,\sigma^2}{\text{argmax}}\log f(\bm y^{(b)}|\bm x,\mlevec)$
  \end{itemize}
\end{itemize}


\noindent
A 95\% Confidence interval for $w$ based on bootstrap is (\textbf{1.716, 1.804}).\\
A 95\% Confidence interval for $\delta$ based on bootstrap is (\textbf{-0.3922, 0.2558}).\\

\noindent
The bootstrapped 95\% confidence intervals contain the intervals constructed
from a Wald-like test based on the Normal approximation. The 95\% confidence
interval for $\delta$ estimated using bootstrap includes 0, while that estimated
using the wald-like test does not.

\subsection*{1d)}
Typically, the Wald-like test intervals tend to be narrower than the truth when
$n$ is small. In any case, the bootstrap interval is more conservative. So in
this situation where the wald interval for $\delta$ does not include 0 but the
bootstrap interval for $\delta$ includes 0, and where the upper bound for
$\delta$ in the wald interval is so close to 0, I would conclude that $\delta$
is \textbf{not significantly different from 0}.

\subsection*{2a)}
\[
\begin{aligned}
  \lambda(\bm x, \bm y) &= \frac{\underset{\lambda_1-\lambda_2 < 0}{\sup} f(\lambda_1, \lambda_2 | \bm x, \bm y)}{\underset{\lambda_1-\lambda_2 \ne 0}{\sup} f(\lambda_1, \lambda_2 | \bm x, \bm y)}\\
\end{aligned}
\]

\noindent
Under the exponential distribution, the parameter values that maximize the likelihoods are the MLE's for 
parameters. In this case, since the two samples are independent, the MLE's are just $\bar x, \bar y$. When
$\bar x < \bar y$, $\lambda(\bm x, \bm y) = 1$ since the numerator and denominators become the same values. 
The non trivial case and case we examine is when $\bar x \ge \bar y$. When $\bar x \ge \bar y$, the parameters
that maximize the likelihood are equal. Then, to solve for the parameters in the constrained case, 
\[
\begin{aligned}
  L&=\lambda_0^{-2n}\exp\p{-\frac{\suml x_i + y_i}{\lambda_0}}\\
  \Rightarrow l=\log L &= -2n\log(\lambda_0) - \frac{\suml x_i + y_i}{\lambda_0}\\
  \\
  l' &:=0 \\
  \Rightarrow -\frac{2n}{\hat\lambda_0} + \frac{\suml x_i + y_i}{\hat\lambda_0^2} &= 0 \\
  \Rightarrow -2n\hat\lambda_0 + \suml x_i + y_i &= 0 \\
  \Rightarrow \hat\lambda_0 &= \frac{\suml x_i + y_i}{2n} =  \frac{\bar x + \bar y}{2}
\end{aligned}
\]
Therefore, to compute the LRT, 
\def\xbar{\bar x}
\def\ybar{\bar y}
\[
\begin{aligned}
  \lambda(\bm x, \bm y) &= \frac{f(\hat\lambda_0, \hat\lambda_0 | \bm x, \bm y)}{f(\bar x, \bar y | \bm x, \bm y)}\\
                        &= \frac{\hat\lambda_0^{-2n}\exp\p{-\frac{\suml x_i + \suml y_i}{\hat\lambda_0}}}{\bar x^{-n} \bar y^{-n}\exp\p{-\suml x_i/\bar x - \suml y_i /\bar y}}\\
                        &= \frac{\hat\lambda_0^{-2n}}{\bar x^{-n} \bar y^{-n}}\\
                        &= \p{\frac{4\bar x \bar y}{(\bar x + \bar y)^2}}^n \\
                        &= \p{\frac{(\bar x + \bar y)^2}{4\bar x \bar y}}^{-n} \\
                        &= \p{\frac{1}{4}\p{1+\frac{\ybar}{\xbar}}\p{1+\frac{\xbar}{\ybar}}}^{-n}\\
                        &= 4^n\p{1+\frac{\ybar}{\xbar}}^{-n}\p{\frac{\ybar/\xbar}{1+\ybar/\xbar}}^n\\
\end{aligned}
\]
Note that under $H_0$, $\frac{\bar Y}{\bar X} = \frac{2\suml
Y_i/(2\lambda_1n)}{2\suml X_i/(2\lambda_1n)} =
\frac{\chi^2_{(2n)}/2n}{\chi^2_{(2n)}/2n} \sim F_{2n,2n}$. Furthermore, it can
be shown the expression above is monotone decreasing in $\ybar/\xbar$ by taking
the derivative w.r.t. the statistic.  So, $\lambda(\bm x,\bm y) < c \iff
\frac{\bar Y}{\bar X} < c$ Therfore, the LRT rejects $H_0$ when $\lambda(\bm x,
\bm y) =
4^n\p{1+\frac{\ybar}{\xbar}}^{-n}\p{\frac{\ybar/\xbar}{1+\ybar/\xbar}}^n > c$,
where $c$ is some constant between 0 and 1. The p-value can be computed as
follows:
\[
\begin{aligned}
  \text{p-value} &= P\p{\Lambda(\bm X,\bm Y) < \lambda(\bm x,\bm y)}\\
                 &= P\p{\frac{\bar Y}{\bar X} < \frac{\ybar}{\xbar}}\\
                 &= P\p{F < \frac{\ybar}{\xbar}}\\
\end{aligned}
\]
where $F\sim F_{2n,2n}$.

\subsection*{2b)}
A wald statistic can be defined as 
\[
\begin{aligned}
  Z_n &= \frac{\bar X - \bar Y}{SE(\bar X - \bar Y)} \\
      &= \frac{\bar X - \bar Y}{\sqrt{SE(X)^2 + SE(Y)^2}}\\
      &= \frac{\bar X - \bar Y}{\sqrt{Var(X)/n + Var(Y)/n}}\\
  \\
  \Rightarrow \text{p-value} = P(Z < z_n) &= \Phi\p{\frac{\bar x - \bar y}{\sqrt{var(x)^2/n + var(y)^2/n}}}
\end{aligned}
\]

%z_stat_wald <- (mean(X) - mean(Y)) / sqrt( (mean(X)^2 + mean(Y)^2) / N)
%(p_val_wald <- pnorm(z_stat_wald,lower=FALSE))


\subsection*{2c)}
A permutation test can be implemented by repeating the following steps many times (e.g. 10000):
\begin{itemize}
  \item Relabel the observations (into one of the two groups)
  \item Compute the difference of the means of two groups
\end{itemize}
Computing the proportion of times the sampled difference in means is less than
the actual observed difference in means will give a p-value for this test.

\subsection*{2d)}
For the lifetime data, the p-values using the LRT, Wald statistic, and
permutation test were computed using the above formulas to be:

\begin{center}
\begin{tabular}{c|ccc}
  & LRT & Wald & Permutation \\
  \hline & \hline & \hline & \hline \\
  p-value  & .045 & .0331 &.0365
\end{tabular}
\end{center}

\noindent
The wald and permutation tests agree. At the 95\% confidence level, we would 
conclude that the mean lifetime of $x$ is greater than that of $y$. 
The LRT p-value is much greater than the other two p-values. 

\subsection*{3a)}
\[
\begin{aligned}
  \beta(\theta) &= P_\theta(T>t_\alpha) \\
  \Rightarrow &= P_\theta(\frac{\sqrt{n} (\bar X - \theta_0) }{S} > t_\alpha) \\
  \Rightarrow &= P_\theta(\frac{\sqrt{n} (\bar X - \theta) }{S} > t_\alpha + \frac{\sqrt{n} (\theta_0-\theta)}{S}) \\
  \Rightarrow &= P_\theta(T_{n-1} > t_\alpha + \frac{\sqrt{n} (\theta_0-\theta)}{S}) \\
\end{aligned}
\]
where $T_{n-1}$ is a $t$-distribution with $n-1$ degrees of freedom. And
$t_\alpha$ is chosen for the appropriate size.  That is $t_\alpha =
\Psi_{n-1}^{-1}(1-\alpha)$, where $\Psi_{n}^{-1}(x)$ is the inverse CDF of the
t distribution with $n$ degrees of freedom.


\subsection*{3b)}
\[
\begin{aligned}
  \beta(\theta) &= P_\theta(T>z_\alpha) \\
  \Rightarrow &= P_\theta(\frac{\sqrt{n} (\bar X - \theta_0) }{S} > z_\alpha) \\
  \Rightarrow &= P_\theta(\frac{\sqrt{n} (\bar X - \theta) }{S} > z_\alpha + \frac{\sqrt{n} (\theta_0-\theta)}{S}) \\
  \Rightarrow &= P_\theta(T_{n-1} > z_\alpha + \frac{\sqrt{n} (\theta_0-\theta)}{S}) \\
\end{aligned}
\]
where $T_{n-1}$ is a $t$-distribution with $n-1$ degrees of freedom. And $z_\alpha$ is chosen for the appropriate size.
That is $z_\alpha = \Phi^{-1}(1-\alpha)$.

\subsection*{3c)}
In Figure \ref{fig:power} below, both power curves are plotted for $n=10,100$, and $1000$. The power of the normal
test is greater than that of the t test for small $n$. As $n$ increases, the powers are approximately equal. At 
$\theta = \theta_0$, the power is $\alpha$ for both curves.

\beginmyfig
  \includegraphics[scale=.5]{../code/output/power.pdf}
  \caption{\small Power function.}
  \label{fig:power}
\endmyfig

\subsection*{3d)}
The T test above is \textbf{not uniformly most powerful} because for small $n$,
the power $\beta(\theta)$ is not always greater than the power
($\beta'(\theta)$) for all $\theta \in \Theta_0$. For example as seen in Figure
$\ref{fig:power}$, when $n=10$, and $\theta > \theta_0$, the power for the test
based on the Normal test statistic is greater than that of the T test
statistic. \\

\noindent
To detect an increase in the mean (w.r.t. $\theta_0$) equivalent to two times
the true standard deviation of the data with probability 0.8, we need to solve
for the sample size under those conditions when the power is 0.8.

\[
\begin{aligned}
  \beta(\theta) &= .8 \\
  \Rightarrow P_\theta(T_{n-1} > t_\alpha + \frac{\sqrt{n} (\theta_0-(\theta_0+2s))}{s}) &= .8\\
  \Rightarrow P_\theta(T_{n-1} > t_\alpha - 2\sqrt{n}) &= .8\\
  \Rightarrow P_\theta(T_{n-1} > t_{.05} - 2\sqrt{n}) &= .8\\
\end{aligned}
\]

\noindent
Since $n$ cannot be solved analytically, we can compute the power function for different values of $n$ to see
when the power is above 0.8.

\begin{table}[ht]
\centering
\begin{tabular}{rc}
  n & Power \\ 
  \hline & \hline \\
  1 & 0.06 \\ 
  2 & 0.18 \\ 
  3 & 0.37 \\ 
  4 & 0.54 \\ 
  5 & 0.66 \\ 
  6 & 0.75 \\ 
  \textbf 7 & \textbf{0.81} \\ 
\end{tabular}
\end{table}

\noindent
When $\mathbf{n=7}$, we can detect detect an increase in the mean equivalent to two
times the true standard deviation of the data with probability 0.8.
\end{document}
