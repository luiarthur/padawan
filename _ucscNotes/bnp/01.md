---
layout: notes
ams: 241
title: First BNP Class
date: "09/24/2015"
dollar: on
---

## Preliminaries
- [Intro](/assets/ams241/bnp_papers/bnp_ams241_intro.pdf)
- [Course Website](http://courses.soe.ucsc.edu/courses/ams241/Fall15/01)
- DP - [Ferguson (1970)](https://projecteuclid.org/euclid.aos/1176342360#abstract)
- Gibbs was first used for BNP, (1988) before general purpose Gibbs was developed.
- *Review papers for final class project*
    - [logistic regression](http://projecteuclid.org/euclid.ba/1437137636)
    - dp
- Presentations of Final Project: 7 December, 8-11 am
- R: "DPpackage" (5 years old), for interest.
- References:
    - [Hanson, Branscum, Johnson](/assets/ams241/bnp_papers/hanson-branscum-johnson.pdf)
    - Ghosh and Ramamoorthi (2004) *Bayesian Nonparametrics*, New York: Springer.
      - Good reference, but no data analysis
    - Hjort, Holmes, Muller, Walker (2010) *Bayesian Nonparametrics*, Camberidge University Press.
    - [Muller, Mitra (2013) "Bayesian nonparametric inference - why and how", *Bayesian Analysis*, 8, 269-360.](http://projecteuclid.org/euclid.ba/1369407550) (**Read tonight**)

## Lecture Notes (2012)
- Found on [Kottas' personal website](https://users.soe.ucsc.edu/~thanos/)

***
## Post-Lecture Thoughts
### Comparison between BNP \& Hierarchical Modeling
I was still having a hard time identifying the differences between a
hierarchical bayesian model and a BNP model. So, I decided to write out the
models and think about the differences. Here are two simple models.

#### BNP:
$$ \begin{array}{rclcl}
    y\_i &|& \theta\_i &\sim& F(\theta\_i) \\\\
    \theta\_i &|& G &\sim& G \\\\
    && G &\sim& \text{DP}(G\_0,\alpha) \\\\
\end{array} $$

#### Hierarchical Model:
$$ \begin{array}{rclcl}
    y\_i &|& \theta\_i &\sim& F(\theta\_i) \\\\
    && \theta\_i &\sim& G \\\\
\end{array} $$

In the BNP model, a possible choice for $F(\theta)$ may be N($\theta,0.1$). $G$
can then be thought of as the distribution for $\theta$. $G$ could be any
distribution that fits the support of $\theta$. $G$ could be N(5,1) -- who
knows?! We don't know. So, we put a prior on $G$ -- The DP with "mean" $G\_0$,
which could be say a N(3,2), and mass parameter $\alpha$. As for the funny
notation in lines 2-3, G is unknown, so it appears several times.  The first
time as the given, the second time as prior the distribution for $\theta$, and
the third time as the distribution being given a prior distribution.
Heuristically, lines 2-3 says that the distribution of the distribution for
$\theta$ is centered at the distribution $G_0$. To be clear, we are not
modeling the mean of or any other parameter associated with $\theta$. We are
modeling the *distribution* of $\theta$. And if you think about it, this is the
notation that makes the most sense. How else could you think of to correctly
express this idea to model the distribution? There is no more concise way.

In the hierarchical model, once again, F($\theta$) could be N($\theta$,0.1),
but $G$ is where the model specification stops. $G$ could be N(3,2). And that
would be the end. Heuristically, $\theta$ could be any value, but we believe it
is centered at 3. End of story.

***
### Reflections of *Why and How of BNP*
First, some preliminaries. Recall that if we have the model
$$ \begin{array}{rclcl}
      y &|& \theta &\sim& \text{Normal}(\theta,1)\\\\
        & & \theta &\sim& \text{Normal}(m,s^2)\\\\
\end{array} $$
or more generally,
$$ \begin{array}{rclcl}
      y &|& \theta &\sim& F(\theta)\\\\
        & & \theta &\sim& G
\end{array} $$
then the posterior $ \[\theta|y\] \propto \[y|\theta\]\[\theta\] $.  The
posterior predictive can be computed as $\[y^\*\] = \int \[ y|\theta^\* \]
\[\theta^\*\]d\theta^\*$, where $\theta^\*$ is the posterior $\theta$.  Or
using different notations, $f(y^\*) = \displaystyle \int f(y|\theta^\*)
g(\theta^\*) d\theta^\* = \displaystyle \int f(y|\theta^\*) dG(\theta^\*)$.
When we see this form, we have the following interpretation:

1. $g(\theta^\*) d\theta^\* \Rightarrow$ simulate draws from $g(\cdot)$
2. $\displaystyle \int f(y|\theta^\*) \Rightarrow$ evaluate the draws at
   $f(y|\theta=\theta^\*)$. The resulting draws evaulated at $f$ would be the
   posterior predictive for $y$.

For our BNP model, 
$$ \begin{array}{rclcl}
    y\_i &|& \theta\_i &\sim& F(\theta\_i) \\\\
    \theta\_i &|& G &\sim& G \\\\
    && G &\sim& \text{DP}(G\_0,\alpha) \\\\
\end{array} $$
we can marginalize $\theta$ such that 
$$ \[ y  \] = \displaystyle\int \[ y | \theta \] \[\theta \] d\theta =
\displaystyle\int \[ y | \theta \] dG(\theta).$$ In case it's not obvious, note
that $\[\theta \] d\theta = g(\theta)d\theta = dG(\theta)$, where $g$ is the
pmf, $G$ is the CDF, and $G \sim DP(G_0,\alpha)$. Now, the next step is to
**learn how to sample from the
[DP](https://en.wikipedia.org/wiki/Dirichlet_process) to get distributions for
$G$**. Then, all we need to do for a basic Gibbs sampler is to draw a
distribution for $G$ from the DP, then draw a $\theta$ from $G$ to fit our
model.

<!-- Left off at Example 1 -->

### R Code for sampling from DP
It is valuable to practice drawing from the DP. While the DP is discrete, and
it is impossible to draw an infinite number of draws, we can use a large number
(here 1000), and see the behavior of the DP. I have provided some R code.
Change `N`,`a`, and `rG` to study the behavior.
<div class = 'code'>
  {% highlight R linenos %}
    {% include code/ams241/01/dp_v1.R %}
  {% endhighlight %}
</div>
![Sampling from the DP](/assets/ams241/01/plots/dp_compare.svg)
