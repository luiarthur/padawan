---
title: "g prior"
date: "02/01/2016"
comments: cucumber
---

Let $\phi$ be the precision parameter. The formulations of g-prior is 
$$
  \beta | \phi \sim N(0, \frac{g}{\phi} \paren{X'X}^{-1}), \pi(\phi) \propto 1/\phi
$$
for $n \approx 100-1000, p \approx 16-30$

$\pi(y | M\_\gamma) = \int~like*prior~d\beta d\phi$

For $M\_{null}$, $R\_\gamma^2 = 0, p\_\gamma=0$. So, bayes factor = $\frac{\pi(y|M\_\gamma)}{\pi(y|M\_{null})}= \frac{(1+g)^{(n-1-p\_\gamma)/2}}{[1+g(1-R\_\gamma^2)]^{(n-1)/2}}$

As $g \rightarrow \infty$, BF($M\_\gamma: M\_{null}$) $\rightarrow 0$. This mean that non-informative prior on $\beta$ will help reject the model even if it is correct. (Bartlett Paradox).

As $R\_\gamma^2 \rightarrow 1$, BF $\rightarrow (1+g)^{-p\_\gamma/2}$

Since $M\_\gamma$ approaches to a model with a perfect fit when $R\_\gamma^2 \rightarrow 1$, BF should go to $\infty$. (Information Paradox)

Both paradoxes found in Langer et. al 2008.

Since fixing $g$ leads to paradoxes, we need to put a prior on $g$.

- Zellner-Siow prior
    - $\pi(g) = \frac{\sqrt{n/2}}{\Gamma(1/2)} g^{-3/2} e^{-n/(2g)}$, $g \gt 0$
    - $\beta | \phi \sim $ multivariate t
- Hyper-g prior (pretty useless prior - nobody uses it)
    - $\pi(g) = \frac{a-2}{2} (1+g)^{-a/2}$, $g \gt 0$
    - $\frac{g}{1+g} \sim Beta(1, \frac{a}{2}-1)$
    - $\pi(g|y,M\_\gamma) = $ ugly...
    - but BF is in closed form (need to do 1 numerical integration)

### Gaussian Process
$y = \mu\_0(x) + \epsilon$, $\mu\_0$ is an unknown function, continuously differentiable up to order $s$.
$\mu\_0^{(\nu)}$ is Lipschitz cont of order $\nu-[\nu]$, $[x]$ is the floor of $x$. Stronger than continuous.

When using GP, you need Lipschitz continuity. 

$y = \mu(x) + \epsilon$, $\mu \sim GP(0,\kappa(.,.; \phi))$. e.g. exponential correlation function $\kappa(x\_i,x\_j;\phi) = e^{-\norm{x\_i-x\_j}\phi}$, matern class of correlation function.

- Likelihood: $y \sim N(\mu,\sigma^2 I)$
- Prior: $\mu(x) \sim N(\mathbf 0, K)$, $K\_{ij} = \kappa(x\_i,x\_j; \phi)$
- Posterior: available in closed form.
    - $p$ is big, model is sparse (not great for GP).
        1. use: $\kappa(x\_i,x\_j;\phi) = e^{-(x\_i-x\_j)'P(x\_i-x\_j)\phi}$. $P$ is a diagonal matrix.
          - put spike and slab prior of diag elements of P.
        2. Try: $y = \mu(\Phi(x)) + \epsilon$, $p=20000, n = 5000$. Raj, et al (2016 JMLR)

### Potential project: 

- Bayesian Lasso, or other shrinkage prior on GP predictors 
