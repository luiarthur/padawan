---
title: "Shrinkage & Regularization"
date: "01/13/2015"
comments: fb
---

# Elastic Net

- Naive Elastic Net does not perform well in practice, because of double penalizing.
- $\beta(\text{enet}) = (1+\lambda\_2)~\beta(\text{naieve enet})$, the same $\lambda\_2$ as previously used.
- `enet` package in R


# Adaptive Lasso

- for $p \gt n$, $\hat\beta\_j$ is the OLS estimate for $y = \beta\_0 + \beta\j x\_j + \epsilon$, $\epsilon \sim N(0,\sigma^2)$, for $j = 1,...,p$. No need to do Bonferroni correction. AL converges to $\beta$ in optimal rate $n^{-1/2}$. $\nu = 2$.
- `polywog` package in R
- always better than lasso. But still has the lasso problems 
  - can't get more than $n$ covariates 
  - can't group variables.


# Group Lasso


# Conclusion:

- there is NO uniformly better method. Have to use the right penalties for the right situations.


## Nonnegative Matrix Factorization

- SNP: 0,1,2 = aa, Aa, AA
- clustering observations, then regression (elastic net). When $p$ is small, they use k-means; but this is lousy when $p$ is large.

