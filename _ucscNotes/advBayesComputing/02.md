---
title: "Shrinkage & Regularization"
date: "01/11/2015"
comments: fb
---

**Convex:**

**Sparsity of $\beta$:** $||\beta||\_0$ is the number of $\beta$ that is 0. This is not convex. Difficult to optimize.

**General penalty:** $||y-X\beta||^2 + \lambda||\beta||\_0$ 

In Ridge, once $\lambda$ is known, the solution for $\beta$ is analytic; not so in lasso. So we need algorithms for estimating $\beta$. 

- LARS (Hastie, Tibshirani, Johnstone): Least angle regression (an iteratively updated algorithm)
    - computationally as quick as forward / backward selection
    - angle b/w predictor and error is computed. Similar predictors are updated similarly, based on angle.
    - does not explore entire model space
    - R: `lars` package
- Coordinate descent algorithm
  - $F(\beta) = ||y-X\beta||^2 + \lambda\psi(\beta)$, where $\psi$ is a convex function
  - use a Gibbs like way to optimize each iteration. Pick starting values close to truth, as with any other convex opt. (like newton-raphson).
  - `glmnet` package. `glmnet` More stable than `lars` package. Could be a package problem... but the two algorithms should give the same estimates. Not stable as in with poor starting values or $p \gt\gt n$, we get convergence problems. In any case, use coordinate descent.
  - `cv.glmnet(X,y)`

**Elastic Net:**

- Short-comings of lasso
  - with lasso, at most $n$ of the $p$ variables are non-zero.
  - since the objective function for lasso is not strictly convex, you may not 
    be able to perform group variable selection.
- penalty function: $J(\beta) = \lambda\_1||\beta||^2 + \lambda\_2||\beta||\_1$
  - hybrid of $l\_1 and l\_2$ penalty
  - strictly convex
  - $l\_1$ part generates a sparse model, $l\_2$ part convex and encourages grouping effect and limitations on number of selected variables.
  - Goals: 1 Group variable selection, 2 possibly select more predictors than sample size
      - goal 2: 
