---
title: Bayesian Interval Estimation
date: 2016-02-02
comments: cucumber
---

## Exam

- one note sheet
- up to jeffreys prior

***

## Bayesian Interval Estimation

A region $C \subset \Theta$ is a $100(1-\alpha)%$ credibility 
if $P(\theta \in C  \|  x) \ge (1-\alpha)$. $1-\alpha$ is the credibility level.

Under a jeffreys prior, the posterior is invariant to one-to-one transformations.

HPD
: For a unimodal distribution, $\int_{c_1}^{c_2}~p(\theta \| x) ~d\theta = (1-\alpha)$ and $c_2-c_1$ is the smallest possible length. For multidimensions distributions, the HPD may be a union of several regions. The heights will be the same.

  - Difficult to compute in practice (in multivariate space)
  - Not invariant under one-to-one transformations

## Bayesian Asymptotics (Chapter 4, Berger; Section 4.7, Robert) 

Result: $x_i \overset{iid}{\sim} p(x_i \| \theta), \theta = (\theta_1,...,\theta_n)'$. For 
$\pi(\theta)$ the prior and $p(\theta \| x)$ the posterior, and the prior and likelihood are twice differentiable around $\hat\theta$ the mle. As $n \rightarrow \infty$, the posterior can be
approximated in 4 ways.

1. $p(\theta \| x) \approx N(\mu^\pi(x),v^\pi(x))$ where $(\mu^\pi(x), v^\pi(x))$ are the posterior mean and covariance.

2. $p(\theta \| x) \approx N_p(\hat\theta^\pi, [I^\pi(x)]^{-1})$, where $\hat\theta^\pi$ is the posterior mode and $[I^\pi(x)]_{ij} = -\paren{\frac{\partial^2}{\partial\theta_i \partial\theta_j} log[p(x \| \theta)\pi(\theta)]}\_{\theta=\hat\theta^\pi}$

3. $p(\theta \| x) \approx N_p(\hat\theta,(\hat{I}(x))^{-1})$, where $\hat I(x)$ is the observed Fishers info $[\hat I^\pi(x)]_{ij} = -\paren{\frac{\partial^2}{\partial\theta_i \partial\theta_j} log[p(x \| \theta)]}\_{\theta=\hat\theta} $

4. $p(\theta \| x) \approx N_p(\hat\theta, (I(\hat\theta))^{-1})$ with $I(\theta)$ the expected Fisher information matrix $[I(x)]_{ij} = -E\paren{\frac{\partial^2}{\partial\theta_i \partial\theta_j} log[p(x \| \theta)]}$

