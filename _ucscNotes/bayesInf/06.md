---
title: Bayesian Interval Estimation
date: 2016-02-02
comments: cucumber
---

## Exam

- one note sheet
- up to jeffreys prior

***

## Bayesian Interval Estimation

A region $C \subset \Theta$ is a $100(1-\alpha)%$ credibility 
if $P(\theta \in C | x) \ge (1-\alpha)$. $1-\alpha$ is the credibility level.

Under a jeffreys prior, the posterior is invariant to one-to-one transformations.

**HPD: ** For a unimodal distribution, $\int\_{c\_1}^{c\_2}~p(\theta|x) ~d\theta = (1-\alpha)$ and $c\_2-c\_1$ is the smallest possible length. For multidimensions distributions, the HPD may be a union of several regions. The heights will be the same.

  - Difficult to compute in practice (in multivariate space)
  - Not invariant under one-to-one transformations

## Bayesian Asymptotics (Chapter 4, Berger; Section 4.7, Robert) 

Result: $x\_i \overset{iid}{\sim} p(x\_i|\theta), \theta = (\theta\_1,...,\theta\_n)'$. For 
$\pi(\theta)$ the prior and $p(\theta|x)$ the posterior, and the prior and likelihood are twice differentiable around $\hat\theta$ the mle. As $n \rightarrow \infty$, the posterior can be
approximated in 4 ways.

1. $p(\theta|x) \approx N(\mu^\pi(x),v^\pi(x))$ where $(\mu^\pi(x), v^\pi(x))$ are the posterior mean and covariance.
2. $p(\theta|x) \approx N\_p(\hat\theta^\pi, [I^\pi(x)]^{-1})$, where $\hat\theta^\pi$ is the posterior mode and $[I^\pi(x)]\_{ij} = -\paren{\frac{\partial^2}{\partial\theta\_i \partial\theta\_j} log[p(x|\theta)\pi(\theta)]}\_{\theta=\hat\theta^\pi} $
3. $p(\theta|x) \approx N\_p(\hat\theta,(\hat{I}(x))^{-1})$, where $\hat I(x)$ is the observed Fishers info $[\hat I^\pi(x)]\_{ij} = -\paren{\frac{\partial^2}{\partial\theta\_i \partial\theta\_j} log[p(x|\theta)]}\_{\theta=\hat\theta} $
4. $p(\theta|x) \approx N\_p(\hat\theta, (I(\hat\theta))^{-1})$ with $I(\theta)$ the expected Fisher information matrix $[I(x)]\_{ij} = -E\paren{\frac{\partial^2}{\partial\theta\_i \partial\theta\_j} log[p(x|\theta)]}$

