---
title: Resources
date: 2016-03-29
comments: cucumber
ams: 256
---

### Notes

- [Course Website](https://ams256-spring16-01.courses.soe.ucsc.edu/)
- [Lecture Notes 1 - Intro & Examples of LM](/assets/ams256/notes/notes1.pdf)
- [Lecture Notes 2 - Intro & Examples of LM](/assets/ams256/notes/notes2.pdf)
- [Lecture Notes 3 - LSE](/assets/ams256/notes/notes3.pdf)
- [Lecture Notes 4 - LSE & Generalized Inverse](/assets/ams256/notes/notes4.pdf)
- [Lecture Notes 5 - LSE & Generalized Inverse](/assets/ams256/notes/notes5.pdf)
- [Lecture Notes 6 - LSE: Imposing Conditions](/assets/ams256/notes/notes6.pdf)
- [Lecture Notes 7 - LSE: Gauss Markov Model](/assets/ams256/notes/notes7.pdf)
- [Lecture Notes 8 - LSE: Distribution Theory](/assets/ams256/notes/notes8.pdf)
- [Lecture Notes 9 - Distribution Theory & Inference](/assets/ams256/notes/notes9.pdf)
- [Lecture Notes 10 - Hypothesis Testing](/assets/ams256/notes/notes10.pdf)
- [Lecture Notes 11 - Hypothesis Testing](/assets/ams256/notes/notes11.pdf)
- [Lecture Notes 12 - Contrasts](/assets/ams256/notes/notes12.pdf)
- [Lecture Notes 13 - Sum of Squares](https://drive.google.com/open?id=0B7Ccueiur0BNay1TZzhEMW04T3c)
- [Lecture Notes 14 - Design of Experiments](https://drive.google.com/open?id=0B7Ccueiur0BNU0F6TUdCSDZ3Zzg)
- [Lecture Notes 15 - Design of Experiments and Regression](https://drive.google.com/open?id=0B7Ccueiur0BNTV9sMDZUNFAzbjQ)
- [Lecture Notes 16 - Regression and Variable Selection](https://drive.google.com/open?id=0B7Ccueiur0BNNFNkZy1MbDBHTkk)
- [Lecture Notes 17 - Variable Selection](https://drive.google.com/open?id=0B7Ccueiur0BNSmJNQ2kzTTktSzg)
- [Lecture Notes 18 - Variable Selection and Diagnostics](https://drive.google.com/open?id=0B7Ccueiur0BNa2xMcTZiS18xdVk)



### Homework

- [Homework 1](/assets/ams256/hw/hw1/hw1.pdf)
- [Homework 1 Solutions](/assets/ams256/hw/hw1/hw1_sol.pdf)
- [Homework 2](/assets/ams256/hw/hw2/hw2.pdf)
- [Homework 2 Solutions](/assets/ams256/hw/hw2/hw2_sol.pdf)
- [Homework 3](/assets/ams256/hw/hw3/hw3.pdf)
- [Homework 3 Solutions](/assets/ams256/hw/hw3/hw3_sol.pdf)
- [Homework 4](/assets/ams256/hw/hw4/hw4.pdf)
- [Homework 4 Solutions](/assets/ams256/hw/hw4/hw4_sol.pdf)
- [Homework 5](/assets/ams256/hw/hw5/hw5.pdf)
- [Homework 5 Solutions](/assets/ams256/hw/hw5/hw5_sol.pdf)
- [Homework 6](/assets/ams256/hw/hw6/hw6.pdf)

### Midterms

- [Midterm 1 Solution](/assets/ams256/exams/exam1-sol.pdf)
- [Midterm 2 Solution](/assets/ams256/exams/exam2-sol.pdf)

***

### Some Terms

Linear models
: $\Rightarrow$ linear in "parameters". That is 

$$
  y = f(X)\beta + \epsilon
$$

In linear models, least Squares = BLUE = MLE, for normal errors.

BLUE
: Best Linear Unbiased Estimator. $\hat\beta = (X'X)^-X'y$ are *linear* in $y$.

ANCOVA
: Analysis of Covariance. Mix of Regressions and ANOVA.

R: `options(contrasts=c("contr.sum","contr.poly"))`

contr.sum = the constraint that $\sum \alpha_i = 0$

Default in R for `lm(y~x)` is to set the first level parameter $\alpha_i= 0$

`lm(y ~ x - 1)` => no intercept

Estimate of $y$ is the same, though estimate of $\hat\alpha$ may be different.

LSE
: $\hat\beta = Q(\beta) = \underset{\beta}{\text{argmin}} (y-X\beta)^T(y-X\beta)$

- find the gradient
- set the gradient = 0

Derivatives of Vectors

  - $\frac{\partial b^Ta}{\partial b} = \frac{\partial a^Tb}{\partial b} = a$
  - $\frac{\partial b^TAb}{\partial b} = (A+A^T)b$

Review: Vector Space, Subspace

Teach Span and collumn space using simple 2 by 2 examples.

Span
: The set of all linear combinations of $x_1,...,x_r \in S$  is called the space spanned by $x_1,...,x_r$. If $M$ is a subspace of $S$ and $M$ equals the space spanned by $x_1,...,x_r$ then $\bc{x_1,...,x_r}$  is called a spanning set for $M$.

column space
: In terms of $y=X\beta$. The column space of $X$ is the set of $y$'s such that there exists a $\beta$ such that $y=X\beta$.

See lecture 2 notes on p. 21 / 64 for the application.

Linearly Dependent
: Let $x_1,...,x_r$ be vectors in $S$. If there exists scalars $\alpha_1,...,\alpha_r$ not all 0, such that $\sum \alpha_i =0_r$ then $x_1,...,x_r$ are linearly dependent. Otherwise, linearly independent.

Basis
: If $M$ is a subspace of $S$ and if $\bc{x_1,...,x_r}\in S$ is a linearly independent spanning set for $M$, then $\bc{x_1,...,x_r}$ is called a basis for $M$. (check)

Rank:

Singular:
